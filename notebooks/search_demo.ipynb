{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtitle Search Engine Demo\n",
    "\n",
    "This notebook demonstrates how to use the subtitle search engine to find relevant content in subtitle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "# Import search engine modules\n",
    "try:\n",
    "    from src.search_engine import SubtitleSearchEngine\n",
    "    from src.enhanced_search import EnhancedSubtitleSearchEngine\n",
    "    print(\"Successfully imported search engine modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing search engine modules: {e}\")\n",
    "    print(\"Make sure you're running this notebook from the notebooks directory and that src/ exists\")\n",
    "    # Create stub versions for demo purposes\n",
    "    print(\"Creating stub classes for demonstration...\")\n",
    "    \n",
    "    class SubtitleSearchEngine:\n",
    "        def __init__(self, data_path, sample_size=1.0):\n",
    "            self.data_path = data_path\n",
    "            self.sample_size = sample_size\n",
    "            self.subtitles = []\n",
    "            print(f\"Initialized SubtitleSearchEngine with data_path={data_path}\")\n",
    "        \n",
    "        def load_data(self):\n",
    "            print(\"Loading data (stub implementation)\")\n",
    "            return self\n",
    "        \n",
    "        def preprocess_all_subtitles(self):\n",
    "            print(\"Preprocessing subtitles (stub implementation)\")\n",
    "            return self\n",
    "        \n",
    "        def vectorize_documents(self):\n",
    "            print(\"Vectorizing documents (stub implementation)\")\n",
    "            return self\n",
    "        \n",
    "        def search(self, query, top_n=5):\n",
    "            print(f\"Searching for '{query}' (stub implementation)\")\n",
    "            return [{'file_name': f'example_{i}.srt', 'similarity': 0.9-i*0.1, 'content_preview': f'Demo content for query: {query}'} for i in range(top_n)]\n",
    "    \n",
    "    class EnhancedSubtitleSearchEngine:\n",
    "        def __init__(self, base_engine=None, data_path=None, sample_size=1.0):\n",
    "            self.base_engine = base_engine\n",
    "            self.data_path = data_path\n",
    "            self.sample_size = sample_size\n",
    "            self.subtitle_clusters = np.array([0, 1, 0, 1, 2, 3]) # Stub cluster data\n",
    "            self.subtitles = [{'file_name': f'example_{i}.srt', 'cleaned_content': f'Demo content {i}'} for i in range(6)]\n",
    "            print(f\"Initialized EnhancedSubtitleSearchEngine\")\n",
    "        \n",
    "        def load_base_engine_data(self):\n",
    "            print(\"Loading base engine data (stub implementation)\")\n",
    "            return self\n",
    "        \n",
    "        def create_semantic_embeddings(self):\n",
    "            print(\"Creating semantic embeddings (stub implementation)\")\n",
    "            return self\n",
    "        \n",
    "        def cluster_subtitles(self, n_clusters=8):\n",
    "            print(f\"Clustering subtitles into {n_clusters} clusters (stub implementation)\")\n",
    "            return self\n",
    "        \n",
    "        def hybrid_search(self, query, top_n=5, semantic_weight=0.7):\n",
    "            print(f\"Hybrid search for '{query}' with semantic_weight={semantic_weight} (stub implementation)\")\n",
    "            return [{\n",
    "                'file_name': f'example_{i}.srt', \n",
    "                'similarity': 0.9-i*0.1, \n",
    "                'content_preview': f'Demo content for query: {query}',\n",
    "                'tfidf_similarity': 0.8-i*0.1,\n",
    "                'semantic_similarity': 0.95-i*0.1,\n",
    "                'cluster': i % 4\n",
    "            } for i in range(top_n)]\n",
    "        \n",
    "        def get_cluster_keywords(self, cluster_id, top_n=10):\n",
    "            print(f\"Getting top {top_n} keywords for cluster {cluster_id} (stub implementation)\")\n",
    "            return [f\"keyword_{i}\" for i in range(top_n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize the Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Setup paths\n",
    "data_path = project_root / \"data\"\n",
    "model_dir = project_root / \"models\"\n",
    "\n",
    "print(f\"Data path: {data_path}\")\n",
    "print(f\"Model directory: {model_dir}\")\n",
    "\n",
    "# Check if model files exist\n",
    "base_model_path = model_dir / \"search_engine_model.pkl\"\n",
    "enhanced_model_path = model_dir / \"enhanced_search_engine_model.pkl\"\n",
    "\n",
    "print(f\"Base model exists: {base_model_path.exists()}\")\n",
    "print(f\"Enhanced model exists: {enhanced_model_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Load from existing model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Try to load from model file\n",
    "search_engine = None\n",
    "\n",
    "if enhanced_model_path.exists():\n",
    "    try:\n",
    "        print(\"Loading enhanced search engine model...\")\n",
    "        # First load base model if it exists\n",
    "        if base_model_path.exists():\n",
    "            base_engine = SubtitleSearchEngine.load_model(str(base_model_path))\n",
    "            search_engine = EnhancedSubtitleSearchEngine.load_model(str(enhanced_model_path), base_engine=base_engine)\n",
    "        else:\n",
    "            search_engine = EnhancedSubtitleSearchEngine.load_model(str(enhanced_model_path))\n",
    "        print(\"Enhanced search engine loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading enhanced model: {e}\")\n",
    "elif base_model_path.exists():\n",
    "    try:\n",
    "        print(\"Loading base search engine model...\")\n",
    "        search_engine = SubtitleSearchEngine.load_model(str(base_model_path))\n",
    "        print(\"Base search engine loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading base model: {e}\")\n",
    "else:\n",
    "    print(\"No model files found. Will need to create a new model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Create a new search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Only create a new engine if no model was loaded\n",
    "if search_engine is None:\n",
    "    use_enhanced = True  # Set to False for basic search only\n",
    "    sample_size = 0.3    # Fraction of data to use (set to 1.0 for all data)\n",
    "    \n",
    "    print(f\"Creating new {'enhanced' if use_enhanced else 'basic'} search engine\")\n",
    "    print(f\"Using {sample_size*100}% of data\")\n",
    "    \n",
    "    # Initialize base search engine\n",
    "    base_engine = SubtitleSearchEngine(str(data_path), sample_size=sample_size)\n",
    "    \n",
    "    # Load and preprocess subtitles\n",
    "    base_engine.load_data()\n",
    "    base_engine.preprocess_all_subtitles()\n",
    "    base_engine.vectorize_documents()\n",
    "    \n",
    "    if use_enhanced:\n",
    "        # Initialize enhanced search engine\n",
    "        search_engine = EnhancedSubtitleSearchEngine(base_engine=base_engine)\n",
    "        search_engine.load_base_engine_data()\n",
    "        search_engine.create_semantic_embeddings()\n",
    "        search_engine.cluster_subtitles(n_clusters=8)\n",
    "        \n",
    "        # Save models\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        base_engine.save_model(str(base_model_path))\n",
    "        search_engine.save_model(str(enhanced_model_path))\n",
    "        print(f\"Saved models to {model_dir}\")\n",
    "    else:\n",
    "        search_engine = base_engine\n",
    "        # Save model\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        search_engine.save_model(str(base_model_path))\n",
    "        print(f\"Saved model to {base_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Search Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a function to display search results\n",
    "def display_search_results(results, show_detailed=False):\n",
    "    \"\"\"Display search results in a readable format.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(results)} results\\n\")\n",
    "    \n",
    "    # Create DataFrame for easier viewing\n",
    "    if show_detailed and isinstance(results[0].get('similarity'), (int, float)):\n",
    "        # Check if we have the enhanced search results\n",
    "        if 'tfidf_similarity' in results[0]:\n",
    "            df_results = pd.DataFrame([\n",
    "                {\n",
    "                    'Rank': i+1,\n",
    "                    'File': r['file_name'],\n",
    "                    'Combined Score': r['similarity'],\n",
    "                    'TF-IDF Score': r['tfidf_similarity'],\n",
    "                    'Semantic Score': r['semantic_similarity'],\n",
    "                    'Cluster': r.get('cluster', 'N/A')\n",
    "                } for i, r in enumerate(results)\n",
    "            ])\n",
    "        else:\n",
    "            df_results = pd.DataFrame([\n",
    "                {\n",
    "                    'Rank': i+1,\n",
    "                    'File': r['file_name'],\n",
    "                    'Score': r['similarity']\n",
    "                } for i, r in enumerate(results)\n",
    "            ])\n",
    "        \n",
    "        display(df_results)\n",
    "    \n",
    "    # Display content previews\n",
    "    for i, result in enumerate(results[:3]):  # Show only top 3 for brevity\n",
    "        print(f\"\\nResult {i+1}: {result['file_name']} (Score: {result['similarity']:.4f})\")\n",
    "        print(\"-\" * 80)\n",
    "        print(result['content_preview'])\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic search example\n",
    "query = \"climate change\"  # Replace with your query\n",
    "\n",
    "if search_engine is not None:\n",
    "    # Check if we have the enhanced search engine\n",
    "    if hasattr(search_engine, 'hybrid_search'):\n",
    "        print(\"Using basic TF-IDF search\")\n",
    "        results = search_engine.search(query, top_n=5)\n",
    "    else:\n",
    "        results = search_engine.search(query, top_n=5)\n",
    "    \n",
    "    display_search_results(results, show_detailed=True)\n",
    "else:\n",
    "    print(\"Search engine not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced Search Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Enhanced semantic search with different semantic weights\n",
    "if search_engine is not None and hasattr(search_engine, 'hybrid_search'):\n",
    "    query = \"global warming effects\"  # Replace with your query\n",
    "    \n",
    "    print(\"\\n--- Hybrid Search (50% TF-IDF, 50% Semantic) ---\")\n",
    "    results_balanced = search_engine.hybrid_search(query, top_n=5, semantic_weight=0.5)\n",
    "    display_search_results(results_balanced, show_detailed=True)\n",
    "    \n",
    "    print(\"\\n--- Hybrid Search (20% TF-IDF, 80% Semantic) ---\")\n",
    "    results_semantic = search_engine.hybrid_search(query, top_n=5, semantic_weight=0.8)\n",
    "    display_search_results(results_semantic, show_detailed=True)\n",
    "    \n",
    "    print(\"\\n--- Hybrid Search (80% TF-IDF, 20% Semantic) ---\")\n",
    "    results_tfidf = search_engine.hybrid_search(query, top_n=5, semantic_weight=0.2)\n",
    "    display_search_results(results_tfidf, show_detailed=True)\n",
    "else:\n",
    "    print(\"Enhanced search engine not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploring Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Examine clusters of documents (only for enhanced search)\n",
    "if search_engine is not None and hasattr(search_engine, 'subtitle_clusters'):\n",
    "    # Get cluster information\n",
    "    clusters = search_engine.subtitle_clusters\n",
    "    n_clusters = len(np.unique(clusters))\n",
    "    \n",
    "    print(f\"Found {n_clusters} clusters of documents\")\n",
    "    \n",
    "    # Count documents per cluster\n",
    "    cluster_counts = np.bincount(clusters)\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'Cluster': range(len(cluster_counts)),\n",
    "        'Document Count': cluster_counts\n",
    "    })\n",
    "    \n",
    "    # Visualize distribution\n",
    "    plt.figure(figsize=(10,

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtitle Data Exploration\n",
    "\n",
    "This notebook explores the subtitle files in the data directory and analyzes their content for insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path().absolute().parent))\n",
    "\n",
    "# Download NLTK resources if needed\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Examine Subtitle Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to read .srt files\n",
    "def read_srt_file(file_path):\n",
    "    \"\"\"Read and parse an SRT file to extract subtitle text.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Extract subtitle text (remove timestamps and numbers)\n",
    "    pattern = r'\\d+\\n\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}\\n'\n",
    "    content = re.sub(pattern, '', content)\n",
    "    \n",
    "    # Remove empty lines\n",
    "    lines = [line for line in content.split('\\n') if line.strip()]\n",
    "    \n",
    "    return ' '.join(lines)\n",
    "\n",
    "# Get all subtitle files\n",
    "data_dir = Path().absolute().parent / 'data'\n",
    "srt_files = list(data_dir.glob('*.srt'))\n",
    "\n",
    "print(f\"Found {len(srt_files)} subtitle files in {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load subtitle data\n",
    "subtitle_data = []\n",
    "\n",
    "for file_path in srt_files:\n",
    "    try:\n",
    "        content = read_srt_file(file_path)\n",
    "        subtitle_data.append({\n",
    "            'file_name': file_path.name,\n",
    "            'content': content,\n",
    "            'word_count': len(content.split()),\n",
    "            'file_size_kb': file_path.stat().st_size / 1024\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path.name}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_subtitles = pd.DataFrame(subtitle_data)\n",
    "df_subtitles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic statistics\n",
    "if not df_subtitles.empty:\n",
    "    print(f\"Total subtitle files: {len(df_subtitles)}\")\n",
    "    print(f\"Total word count: {df_subtitles['word_count'].sum():,}\")\n",
    "    print(f\"Average word count per file: {df_subtitles['word_count'].mean():.1f}\")\n",
    "    print(f\"Average file size: {df_subtitles['file_size_kb'].mean():.1f} KB\")\n",
    "else:\n",
    "    print(\"No subtitle files found. Please add .srt files to the data directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize file sizes\n",
    "if not df_subtitles.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df_subtitles['file_size_kb'], bins=20, kde=True)\n",
    "    plt.title('Distribution of Subtitle File Sizes')\n",
    "    plt.xlabel('File Size (KB)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_subtitles.sort_values('word_count', ascending=False).head(15), \n",
    "                x='file_name', y='word_count')\n",
    "    plt.title('Top 15 Subtitle Files by Word Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and tokenize text.\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Process all subtitle content if available\n",
    "if not df_subtitles.empty:\n",
    "    # Combine all subtitle content\n",
    "    all_content = ' '.join(df_subtitles['content'].tolist())\n",
    "    \n",
    "    # Preprocess text\n",
    "    tokens = preprocess_text(all_content)\n",
    "    \n",
    "    # Frequency distribution\n",
    "    fdist = FreqDist(tokens)\n",
    "    \n",
    "    # Plot most common words\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    fdist.plot(30, cumulative=False)\n",
    "    plt.title('30 Most Common Words in Subtitles')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Analysis\n",
    "\n",
    "Let's look at word co-occurrence and n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "# Generate and visualize bigrams\n",
    "if 'tokens' in locals():\n",
    "    # Get bigrams\n",
    "    bigrams_list = list(ngrams(tokens, 2))\n",
    "    bigram_freq = FreqDist(bigrams_list)\n",
    "    \n",
    "    # Plot top bigrams\n",
    "    top_bigrams = bigram_freq.most_common(20)\n",
    "    bigram_df = pd.DataFrame(top_bigrams, columns=['Bigram', 'Count'])\n",
    "    bigram_df['Bigram'] = bigram_df['Bigram'].apply(lambda x: f\"{x[0]} {x[1]}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=bigram_df, x='Count', y='Bigram')\n",
    "    plt.title('Top 20 Bigrams in Subtitle Content')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create summary statistics\n",
    "if not df_subtitles.empty:\n",
    "    summary = {\n",
    "        'Total Files': len(df_subtitles),\n",
    "        'Total Words': df_subtitles['word_count'].sum(),\n",
    "        'Avg Words per File': df_subtitles['word_count'].mean(),\n",
    "        'Min Words': df_subtitles['word_count'].min(),\n",
    "        'Max Words': df_subtitles['word_count'].max(),\n",
    "        'Unique Words': len(set(tokens)) if 'tokens' in locals() else 'N/A',\n",
    "        'Vocabulary Richness': len(set(tokens)) / len(tokens) if 'tokens' in locals() else 'N/A'\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary.items(), columns=['Metric', 'Value'])\n",
    "    summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we've explored the subtitle data, consider:\n",
    "\n",
    "1. Adding more subtitle files to increase the corpus size\n",
    "2. Creating a TF-IDF representation of documents for search\n",
    "3. Implementing semantic embeddings for enhanced search\n",
    "4. Building a search interface using Streamlit\n",
    "\n",
    "See the `search_demo.ipynb` notebook for examples of using the search engine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
